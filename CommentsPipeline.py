import logging
import sys
import os
import asyncio
from typing import List, Iterator, Callable
from pydantic import BaseModel
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate

logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))

class Pipeline:
    class Valves(BaseModel):
        MODEL_ID: str = "gpt-4o"
        TEMPERATURE: float = 0.5
        MAX_TOKENS: int = 2000
        OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY", "")

    def __init__(self):
        self.name = "Public Consultation Comment Analyzer"
        self.valves = self.Valves()

    async def on_startup(self):
        logging.info("Pipeline is warming up...")

    async def on_shutdown(self):
        logging.info("Pipeline is shutting down...")

    async def make_request_with_retry(self, fn: Callable[[], Iterator[str]], retries=3) -> Iterator[str]:
        for attempt in range(retries):
            try:
                return fn()
            except Exception as e:
                logging.error(f"Attempt {attempt + 1} failed: {e}")
                if attempt + 1 == retries:
                    raise
                await asyncio.sleep(2 ** attempt)

    def pipe(
        self, user_message: str, model_id: str, messages: List[dict], body: dict
    ) -> Iterator[str]:

        system_message = """
**Ð Ð¾Ð»ÑŒ:** Ð’Ñ‹ â€” Ð°Ð½Ð°Ð»Ð¸Ñ‚Ð¸Ðº Ð¾Ð±Ñ‰ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ñ… ÐºÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ñ†Ð¸Ð¹ Ð¿Ñ€Ð¸ ÐœÐ¸Ð½Ð¸ÑÑ‚ÐµÑ€ÑÑ‚Ð²Ðµ ÑŽÑÑ‚Ð¸Ñ†Ð¸Ð¸ ÐšÐ°Ð·Ð°Ñ…ÑÑ‚Ð°Ð½Ð°. Ð’Ð°ÑˆÐ° Ð·Ð°Ð´Ð°Ñ‡Ð° â€” Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸Ð¸ Ð³Ñ€Ð°Ð¶Ð´Ð°Ð½ Ðº Ð·Ð°ÐºÐ¾Ð½Ð¾Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°Ð¼ Ð¸ Ð½Ð° Ð¸Ñ… Ð¾ÑÐ½Ð¾Ð²Ðµ Ñ„Ð¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ Ð¿Ð¾ Ð´Ð¾Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐµ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ñ€ÐµÐ´Ð°ÐºÑ†Ð¸Ð¸ Ð·Ð°ÐºÐ¾Ð½Ð°.

**Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚ Ð¾Ñ‚Ð²ÐµÑ‚Ð°:**  
Ð’Ð°Ñˆ Ð¾Ñ‚Ð²ÐµÑ‚ Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð¸Ð¼ÐµÑ‚ÑŒ ÑÑ‚Ñ€Ð¾Ð³Ð¾ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÑƒÑŽ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ:

---

**Ð’ÑÑ‚ÑƒÐ¿Ð»ÐµÐ½Ð¸Ðµ:**  
ÐšÑ€Ð°Ñ‚ÐºÐ¾ Ð¾Ð±ÑŠÑÑÐ½Ð¸Ñ‚Ðµ Ñ†ÐµÐ»ÑŒ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð¸ Ð¿ÐµÑ€ÐµÑ…Ð¾Ð´ Ðº ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ðµ.

---

### **1. ÐšÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸Ð¹: "[Ð²ÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸Ð¹]"**  
#### ðŸ”¹ **ÐÐ½Ð°Ð»Ð¸Ð·**  
[ÐšÑ€Ð°Ñ‚ÐºÐ¸Ð¹ Ð°Ð½Ð°Ð»Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ñ€Ð°Ð·Ð±Ð¾Ñ€ Ð¼Ð½ÐµÐ½Ð¸Ñ, Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ñ‹Ðµ Ð¾Ð¿Ð°ÑÐµÐ½Ð¸Ñ, Ð¸Ð½Ñ‚ÐµÑ€ÐµÑÑ‹, Ð½ÐµÐ´Ð¾Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ.]

#### ðŸ”¹ **Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ð¸**  
- [ÐŸÐµÑ€ÐµÑ‡ÐµÐ½ÑŒ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ñ…, ÐºÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ‚Ð¸Ð²Ð½Ñ‹Ñ… Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹ Ð¿Ð¾ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð¸Ñ€Ð¾Ð²ÐºÐµ Ð¸Ð»Ð¸ Ð¿Ð¾ÑÑÐ½ÐµÐ½Ð¸ÑŽ.]

---

(ÐŸÐ¾Ð²Ñ‚Ð¾Ñ€Ð¸Ñ‚ÑŒ Ð±Ð»Ð¾Ðº Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸Ñ)

---

### **Ð˜Ñ‚Ð¾Ð³: ÐºÐ°Ðº ÑƒÑ‡ÐµÑÑ‚ÑŒ Ð¼Ð½ÐµÐ½Ð¸Ðµ Ð³Ñ€Ð°Ð¶Ð´Ð°Ð½?**  
âœ” [Ð¡Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€ÑƒÐ¹Ñ‚Ðµ Ð¸Ñ‚Ð¾Ð³Ð¾Ð²Ñ‹Ðµ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ Ð² Ð²Ð¸Ð´Ðµ Ð¼Ð°Ñ€ÐºÐµÑ€Ð¾Ð².]

**Ð’Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ:**  
â€“ Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ Ð¾Ñ„Ð¸Ñ†Ð¸Ð°Ð»ÑŒÐ½Ð¾-Ð´ÐµÐ»Ð¾Ð²Ð¾Ð¹ ÑÑ‚Ð¸Ð»ÑŒ.  
â€“ ÐšÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸Ð¸ Ð³Ñ€Ð°Ð¶Ð´Ð°Ð½ Ð²ÑÐµÐ³Ð´Ð° Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÑŽÑ‚ÑÑ Ñ ÑƒÐ²Ð°Ð¶ÐµÐ½Ð¸ÐµÐ¼.  
â€“ Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ð¸ Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð±Ñ‹Ñ‚ÑŒ ÐºÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¼Ð¸ Ð¸ Ñ€ÐµÐ°Ð»Ð¸ÑÑ‚Ð¸Ñ‡Ð½Ñ‹Ð¼Ð¸.  
â€“ ÐÐµ ÑÐ¾ÐºÑ€Ð°Ñ‰Ð°Ð¹Ñ‚Ðµ Ð¸ Ð½Ðµ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÑÐ¹Ñ‚Ðµ ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸Ð¸ â€” ÐºÐ°Ð¶Ð´Ñ‹Ð¹ Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ‹Ñ‚ÑŒ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾ Ð¿Ñ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½.
"""

        model = ChatOpenAI(
            api_key=self.valves.OPENAI_API_KEY,
            model=self.valves.MODEL_ID,
            temperature=self.valves.TEMPERATURE,
            streaming=True
        )

        prompt = ChatPromptTemplate.from_messages([
            SystemMessagePromptTemplate.from_template(system_message),
            HumanMessagePromptTemplate.from_template("{user_input}")
        ])

        formatted_messages = prompt.format_messages(user_input=user_message)

        def generate_stream() -> Iterator[str]:
            for chunk in model.stream(formatted_messages):
                content = getattr(chunk, "content", None)
                if content:
                    logging.debug(f"Model chunk: {content}")
                    yield content

        return asyncio.run(self.make_request_with_retry(generate_stream))
