import logging
import sys
import os
import asyncio
import re
import httpx
import mimetypes
import base64
from typing import List, Dict, Iterator, Callable

from pydantic import BaseModel
from openai import OpenAI
from PIL import Image  # noqa: F401 ‚Äì kept for future image analysis extensions
import fitz  # PyMuPDF ‚Äì PDF ‚Üí text
import docx2txt  # .docx ‚Üí text

###############################################################################
# Logging
###############################################################################
logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
logger = logging.getLogger(__name__)
logger.addHandler(logging.StreamHandler(stream=sys.stdout))

###############################################################################
# Helper utilities
###############################################################################

def clean_html(text: str) -> str:
    """Very-light HTML ‚Üí plain-text cleaner (tags + extra whitespace)."""
    text = re.sub(r"<[^>]+>", " ", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()


async def web_search(query: str) -> List[Dict[str, str]]:
    """Call **OpenAI web_search_preview** tool and return a minimal result list.

    The endpoint returns its own textual summary; we wrap it in a pseudo-result so
    it can be fed back into the prompt alongside a Google link for manual review.
    """
    try:
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        rsp = client.responses.create(
            model="gpt-4.1",
            tools=[{"type": "web_search_preview"}],
            input=query,
        )
        return [
            {
                "title": "OpenAI web search preview",  # stable placeholder
                "link": "https://www.google.com/search?q=" + query.replace(" ", "+"),
                "snippet": rsp.output_text,
            }
        ]
    except Exception as exc:
        logger.warning("OpenAI web_search_preview error: %s", exc)
        return []


async def open_url(url: str) -> str:
    """Fetch raw page text (best-effort) for richer context if needed."""
    headers = {"User-Agent": "Mozilla/5.0 (compatible; CommentBot/1.0)"}
    try:
        async with httpx.AsyncClient(timeout=30, headers=headers) as client:
            r = await client.get(url, follow_redirects=True)
            r.raise_for_status()
            return clean_html(r.text)[:15000]  # truncate to 15k chars
    except Exception as exc:
        logger.warning("open_url error for %s: %s", url, exc)
        return f"__FETCH_ERROR__: {exc}"

###############################################################################
# Pipeline
###############################################################################


class Pipeline:
    """OpenWebUI-compatible pipeline that:

    ‚Ä¢ Accepts arbitrary files/images (PDF, DOCX, images) and extracts their text
      into *body["file_text"]* via *inlet*.
    ‚Ä¢ Performs a lightweight OpenAI-powered web search for public comments that
      mention the user‚Äôs query and feeds those snippets to the model.
    ‚Ä¢ Streams the analysis back to OpenWebUI.
    """

    class Valves(BaseModel):
        MODEL_ID: str = "gpt-4o"
        TEMPERATURE: float = 0.5
        MAX_TOKENS: int = 2000
        OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY", "")

    def __init__(self):
        self.name = "Public Consultation Comment Analyzer"
        self.valves = self.Valves()
        self.client = OpenAI(api_key=self.valves.OPENAI_API_KEY)

    # ---------------------------------------------------------------------
    # Lifecycle hooks (optional in OpenWebUI but kept for completeness)
    # ---------------------------------------------------------------------
    async def on_startup(self):
        logger.info("%s pipeline warming up‚Ä¶", self.name)

    async def on_shutdown(self):
        logger.info("%s pipeline shutting down‚Ä¶", self.name)

    # ------------------------------------------------------------------
    # Resilient execution wrapper (same semantics as in original template)
    # ------------------------------------------------------------------
    async def _run_with_retry(self, coro_factory: Callable[[], asyncio.Future], retries: int = 3):
        for attempt in range(1, retries + 1):
            try:
                return await coro_factory()
            except Exception as exc:
                logger.error("Attempt %d failed: %s", attempt, exc)
                if attempt == retries:
                    raise
                await asyncio.sleep(2 ** (attempt - 1))

    # ------------------------------------------------------------------
    # Inlet ‚Äì extract text from any supported file types and stash to body
    # ------------------------------------------------------------------
    async def inlet(self, body: dict, user: dict | None = None):  # noqa: D401 ‚Äì OWUI signature
        """Extract plain text from PDFs, DOCX files, and images.

        The extracted text is placed in `body["file_text"]` so it is available
        inside *pipe()*.
        """
        extracted_chunks: List[str] = []
        for f in body.get("files", []):
            try:
                # ---- Download binary content from WebUI-provided presigned URL
                content_url = f["url"] + "/content"
                async with httpx.AsyncClient(timeout=30) as c:
                    resp = await c.get(content_url)
                    resp.raise_for_status()
                    content = resp.content

                # ---- Detect MIME type and route to an extractor
                mime = f.get("mime_type") or mimetypes.guess_type(f.get("name", ""))[0]
                if mime == "application/pdf":
                    doc = fitz.open(stream=content, filetype="pdf")
                    extracted_chunks.append("\n".join(p.get_text() for p in doc))
                    doc.close()
                elif mime == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
                    tmp_name = "_tmp_uploaded.docx"
                    with open(tmp_name, "wb") as tmp:
                        tmp.write(content)
                    extracted_chunks.append(docx2txt.process(tmp_name))
                    os.remove(tmp_name)
                elif mime and mime.startswith("image/"):
                    b64 = base64.b64encode(content).decode()
                    rsp = self.client.chat.completions.create(
                        model=self.valves.MODEL_ID,
                        messages=[
                            {
                                "role": "user",
                                "content": [
                                    {"type": "text", "text": "–ò–∑–≤–ª–µ–∫–∏ (OCR) —Ç–µ–∫—Å—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –±–µ–∑ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏."},
                                    {"type": "image_url", "image_url": {"url": f"data:{mime};base64,{b64}"}},
                                ],
                            }
                        ],
                        temperature=0.0,
                        max_tokens=1024,
                    )
                    extracted_chunks.append(rsp.choices[0].message.content.strip())
                else:
                    logger.warning("Unsupported MIME type %s ‚Äì skipped", mime)
            except Exception as exc:
                logger.warning("Failed to extract from %s: %s", f.get("name"), exc)

        if extracted_chunks:
            body["file_text"] = "\n\n".join(extracted_chunks)
        return body  # WebUI expects the (possibly modified) body back

    # ------------------------------------------------------------------
    # Main pipeline logic ‚Äì called for each user turn
    # ------------------------------------------------------------------
    def pipe(
        self,
        user_message: str,
        model_id: str,
        messages: List[dict],
        body: dict | None = None,
    ) -> Iterator[str]:
        body = body or {}
        # --------------------------------------------------------------
        # 1Ô∏è‚É£ Augment user message with extracted file text (if any)
        # --------------------------------------------------------------
        if body.get("file_text"):
            user_message += "\n\n–¢–µ–∫—Å—Ç –∏–∑ –ø—Ä–∏–∫—Ä–µ–ø–ª—ë–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:\n" + body["file_text"]

        # --------------------------------------------------------------
        # 2Ô∏è‚É£ Fetch public commentary snippets via OpenAI search preview
        # --------------------------------------------------------------
        async def _collect_search_context():
            results = await web_search(user_message)
            if not results:
                return ""
            formatted = "\n".join(
                f"{i+1}. {r['snippet']} (–∏—Å—Ç–æ—á–Ω–∏–∫: {r['link']})" for i, r in enumerate(results)
            )
            return "–ù–∞–π–¥–µ–Ω–Ω—ã–µ –≤ –æ—Ç–∫—Ä—ã—Ç–æ–º –¥–æ—Å—Ç—É–ø–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏:\n" + formatted

        search_context = asyncio.run(self._run_with_retry(_collect_search_context))

        # --------------------------------------------------------------
        # 3Ô∏è‚É£ Compose prompt (the original system instructions + context)
        # --------------------------------------------------------------
        system_message = (
            "**–†–æ–ª—å:** –í—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–π –ø—Ä–∏ –ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–µ —é—Å—Ç–∏—Ü–∏–∏ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–∞. "
            "–í–∞—à–∞ –∑–∞–¥–∞—á–∞ ‚Äî –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –≥—Ä–∞–∂–¥–∞–Ω –∫ –∑–∞–∫–æ–Ω–æ–ø—Ä–æ–µ–∫—Ç–∞–º, –≤—ã—è–≤–ª—è—Ç—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏ –∫–ª—é—á–µ–≤—ã–µ —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏, "
            "–∏ –Ω–∞ –∏—Ö –æ—Å–Ω–æ–≤–µ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –¥–æ—Ä–∞–±–æ—Ç–∫–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Ä–µ–¥–∞–∫—Ü–∏–∏ –∑–∞–∫–æ–Ω–∞.\n\n"
            "---\n\n"
            "## üî¢ **–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ (—Å—Ç—Ä—É–∫—Ç—É—Ä–∞):**\n\n"
            "### **–í—Å—Ç—É–ø–ª–µ–Ω–∏–µ**\n"
            "–ö—Ä–∞—Ç–∫–æ –æ–ø–∏—à–∏—Ç–µ —Ü–µ–ª—å –∞–Ω–∞–ª–∏–∑–∞, –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∏ –ø–æ–¥—Ö–æ–¥.\n\n"
            "---\n\n"
            "### **1. –ö–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∏ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑**\n"
            "- **–í—Å–µ–≥–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤:** [—á–∏—Å–ª–æ]\n"
            "- **–ü–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö:** [—á–∏—Å–ª–æ] ([%])\n"
            "- **–ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö:** [—á–∏—Å–ª–æ] ([%])\n"
            "- **–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã—Ö:** [—á–∏—Å–ª–æ] ([%])\n\n"
            "#### **–û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–µ–º—ã:**\n"
            "- ‚úî –¢–µ–º–∞ 1 (—É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è –≤ X% –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤)\n"
            "- ‚úî –¢–µ–º–∞ 2 ‚Ä¶\n\n"
            "#### **–û—Å–Ω–æ–≤–Ω—ã–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –æ–ø–∞—Å–µ–Ω–∏—è:**\n"
            "- ‚ö† –¢–µ–º–∞ 1 (–≤ Y% –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤)\n"
            "- ‚ö† –¢–µ–º–∞ 2 ‚Ä¶\n\n"
            "#### **–û–±—â–∏–µ —Ç—Ä–µ–Ω–¥—ã:**\n"
            "- –ú–Ω–æ–∂–µ—Å—Ç–≤–æ —Å—Ö–æ–∂–∏—Ö –æ–ø–∞—Å–µ–Ω–∏–π –ø–æ —Ç–µ–º–µ...\n"
            "- –ü–æ–≤—Ç–æ—Ä—è—é—â–∞—è—Å—è –ø—Ä–æ—Å—å–±–∞ –æ‚Ä¶\n\n"
            "---\n\n"
            "### **2. –ü–æ –∫–∞–∂–¥–æ–º—É –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—é:**\n\n"
            "#### **–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: \"[–≤—Å—Ç–∞–≤–∏—Ç—å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π]\"**\n"
            "üîπ **–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:** (–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π / –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π / –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π)  \n"
            "üîπ **–ê–Ω–∞–ª–∏–∑:**  \n"
            "[–ö—Ä–∞—Ç–∫–∏–π —Ä–∞–∑–±–æ—Ä —Å—É—Ç–∏, –º–æ—Ç–∏–≤–∞—Ü–∏–∏, –ª–æ–≥–∏–∫–∏, –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤]\n\n"
            "üîπ **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**  \n"
            "- [–ß—ë—Ç–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: –ø–æ—è—Å–Ω–∏—Ç—å, –∏–∑–º–µ–Ω–∏—Ç—å, —É—á–µ—Å—Ç—å, –æ—Ç–∫–ª–æ–Ω–∏—Ç—å –∏ –æ–±–æ—Å–Ω–æ–≤–∞—Ç—å]\n\n"
            "---\n\n"
            "(–ø–æ–≤—Ç–æ—Ä–∏—Ç—å –±–ª–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è)\n\n"
            "---\n\n"
            "### **3. –ò—Ç–æ–≥–æ–≤–æ–µ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ**\n"
            "- –û–±–æ–±—â–∏—Ç–µ –∫–ª—é—á–µ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –≤—Å–µ—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤.\n"
            "- –í—ã–¥–µ–ª–∏—Ç–µ, –∫–∞–∫–∏–µ –ø—Ä–∞–≤–∫–∏ —Å—Ç–æ–∏—Ç –æ–±—Å—É–¥–∏—Ç—å –∏–ª–∏ –ø—Ä–∏–Ω—è—Ç—å.\n"
            "- –ï—Å–ª–∏ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –º–Ω–µ–Ω–∏–π –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ ‚Äî –ø—Ä–µ–¥–ª–æ–∂–∏—Ç–µ –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ–∞–≥–∏—Ä–æ–≤–∞–Ω–∏—é (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–∞–∑—ä—è—Å–Ω–µ–Ω–∏—è, —Å–µ—Å—Å–∏—è Q&A, —Å–º—è–≥—á–µ–Ω–∏–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫).\n\n"
            "---\n\n"
            "## üß† –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:\n\n"
            "- –°—Ç–∏–ª—å: –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π, —É–≤–∞–∂–∏—Ç–µ–ª—å–Ω—ã–π, –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π\n"
            "- –ö–∞–∂–¥—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ\n"
            "- –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π—Ç–µ –º–Ω–µ–Ω–∏—è ‚Äî —Ç–æ–ª—å–∫–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n"
            "- –ï—Å–ª–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –º–∞–ª–æ ‚Äî –∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ\n"
            "- –ù–µ –∏–∑–±–µ–≥–∞–π—Ç–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: —É–∫–∞–∑—ã–≤–∞–π—Ç–µ –ø—Ä–æ—Ü–µ–Ω—Ç—ã, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ, –¥–∏–Ω–∞–º–∏–∫—É"
        )

        # Assemble chat messages. We pass the **search context** as an assistant
        # turn so the model treats it as background knowledge, not a direct user
        # instruction.
        chat_messages = [
            {"role": "system", "content": system_message},
            {"role": "assistant", "content": search_context},
            {"role": "user", "content": user_message},
        ]

        # --------------------------------------------------------------
        # 4Ô∏è‚É£ Helper: stream OpenAI response and yield chunks to WebUI
        # --------------------------------------------------------------
        async def _stream_response():
            response = await self._run_with_retry(
                lambda: self.client.chat.completions.create(
                    model=self.valves.MODEL_ID,
                    messages=chat_messages,
                    temperature=self.valves.TEMPERATURE,
                    max_tokens=self.valves.MAX_TOKENS,
                    stream=True,
                )
            )
            async for chunk in response:  # type: ignore ‚Äì `response` is an async iterator
                delta = chunk.choices[0].delta
                if delta and delta.content:
                    yield delta.content

        # --------------------------------------------------------------
        # 5Ô∏è‚É£ Bridge async-generator ‚Üí sync iterator (OpenWebUI requirement)
        # --------------------------------------------------------------
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        agen = _stream_response()
        try:
            while True:
                content = loop.run_until_complete(agen.__anext__())
                yield content
        except StopAsyncIteration:
            pass
        finally:
            loop.run_until_complete(loop.shutdown_asyncgens())
            loop.close()
