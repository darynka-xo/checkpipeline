import logging
import sys
import os
import asyncio
from typing import List, Iterator, Callable
from pydantic import BaseModel
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores.pgvector import PGVector
import json

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.StreamHandler(sys.stderr),          # ‚ñ∫ stderr
        logging.FileHandler("debate_pipeline.log")  # ‚ñ∫ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –≤ —Ñ–∞–π–ª
    ]
)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))



class Pipeline:
    class Valves(BaseModel):
        MODEL_ID: str = "gpt-4o"
        TEMPERATURE: float = 0.5
        MAX_TOKENS: int = 1500
        OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY", "")

    def __init__(self):
        self.name = "Debate Pipeline"
        self.valves = self.Valves()

    async def on_startup(self):
        logging.info("Pipeline is warming up...")

    async def on_shutdown(self):
        logging.info("Pipeline is shutting down...")

    async def make_request_with_retry(self, fn: Callable[[], Iterator[str]], retries=3) -> Iterator[str]:
        for attempt in range(retries):
            try:
                return fn()
            except Exception as e:
                logging.error(f"Attempt {attempt + 1} failed: {e}")
                if attempt + 1 == retries:
                    raise
                await asyncio.sleep(2 ** attempt)

    def pipe(self, user_message: str, model_id: str, messages: List[dict], body: dict) -> Iterator[str]:
        async def run_pipeline():
            embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")
            vectorstore = PGVector(
                connection_string=os.getenv("PGVECTOR_URL", "postgresql://admin:tester123@host.docker.internal:5432/postgres"),
                collection_name="laws_chunks_ru",
                embedding_function=embeddings
            )

            retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 5})
            docs = retriever.get_relevant_documents(user_message)
            legal_context = "\n\n".join([f"- {doc.page_content}" for doc in docs])

            # System message with injected legal context
            system_message = f"""
    **–†–æ–ª—å**: –í—ã - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–µ–±–∞—Ç–æ–≤, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π—Å—è –Ω–∞ –≥–ª—É–±–æ–∫–æ–º –∏ –±–µ—Å–ø—Ä–∏—Å—Ç—Ä–∞—Å—Ç–Ω–æ–º —Ä–∞–∑–±–æ—Ä–µ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –∞–∫—Ç–æ–≤ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–∞.
    
    **–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤**:
    {legal_context}
    
    **–¶–µ–ª—å –∞–Ω–∞–ª–∏–∑–∞**:
    1. –ü—Ä–æ–≤–µ—Å—Ç–∏ –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω—é—é —ç–∫—Å–ø–µ—Ä—Ç–∏–∑—É –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∞—Ä–≥—É–º–µ–Ω—Ç–∞
    2. –û—Ü–µ–Ω–∏—Ç—å –ª–æ–≥–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º
    3. –í—ã—è–≤–∏—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª–∞–±–æ—Å—Ç–∏ –∏ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è
    
    **–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –∞–Ω–∞–ª–∏–∑—É**:
    
    #### 1. –†–∞–∑–¥–µ–ª "–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–ø–∏—Å–∫–∞"
    - –ü–æ–ª–Ω—ã–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–∑–±–æ—Ä –∞—Ä–≥—É–º–µ–Ω—Ç–∞
    - –û—Ü–µ–Ω–∫–∞ –ª–æ–≥–∏—á–µ—Å–∫–æ–π —Å–≤—è–∑–Ω–æ—Å—Ç–∏ –∏ –∞—Ä–≥—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
    - –í—ã—è–≤–ª–µ–Ω–∏–µ —Å–∫—Ä—ã—Ç—ã—Ö –ø–æ—Å—ã–ª–æ–∫ –∏ –Ω–µ—è–≤–Ω—ã—Ö –¥–æ–ø—É—â–µ–Ω–∏–π
    - –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∑–∏—Å–∞
    
    #### 2. –†–∞–∑–¥–µ–ª "–û—Ç—á–µ—Ç –æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –∞–∫—Ç–∞–º"
    - –î–µ—Ç–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–∂–¥–æ–≥–æ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–º –Ω–æ—Ä–º–∞–º
    - –¢–æ—á–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –∏ –ø—É–Ω–∫—Ç—ã –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    - –ö–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π (minor/significant)
    - –ü—Ä–∞–≤–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –Ω–∞—Ä—É—à–µ–Ω–∏–π
    
    #### 3. –†–∞–∑–¥–µ–ª "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é"
    - –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –¥–æ—Ä–∞–±–æ—Ç–∫–µ –∞—Ä–≥—É–º–µ–Ω—Ç–∞—Ü–∏–∏
    - –ú–µ—Ç–æ–¥–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É—Å–∏–ª–µ–Ω–∏—é —Å–ª–∞–±—ã—Ö –ø–æ–∑–∏—Ü–∏–π
    - –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∏ –ø–æ–¥—Ö–æ–¥—ã
    - –°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —É–±–µ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    """

            model = ChatOpenAI(
                api_key=self.valves.OPENAI_API_KEY,
                model=self.valves.MODEL_ID,
                temperature=self.valves.TEMPERATURE,
                streaming=True
            )

            prompt = ChatPromptTemplate.from_messages([
                SystemMessagePromptTemplate.from_template(system_message),
                HumanMessagePromptTemplate.from_template("{user_input}")
            ])
            formatted_messages = prompt.format_messages(user_input=user_message)

            def stream_model() -> Iterator[str]:
                for chunk in model.stream(formatted_messages):
                    content = getattr(chunk, "content", None)
                    if content:
                        logging.debug(f"Model chunk: {content}")
                        yield json.dumps({"content": content})  # ‚úÖ JSON –¥–ª—è OpenWebUI

            # üîÅ await-–∏–º —Ñ—É–Ω–∫—Ü–∏—é, –≤–æ–∑–≤—Ä–∞—â–∞—é—â—É—é –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä
            async def run():
                return self.make_request_with_retry(stream_model)

            # üëá –∑–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–¥ –∏ –ø–æ–ª—É—á–∞–µ–º –∏—Ç–µ—Ä–∞—Ç–æ—Ä
            return await run()

        # üëâ –∑–∞–ø—É—Å–∫–∞–µ–º –≤—Å—ë —ç—Ç–æ –≤ asyncio.run
        return asyncio.run(run_pipeline())
