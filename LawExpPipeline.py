import logging
import sys
import os
import asyncio
from typing import List, Iterator, Callable, Any, Dict
from pydantic import BaseModel
import httpx
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate

logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))


class Pipeline:
    class Valves(BaseModel):
        MODEL_ID: str = "gpt-4o"
        TEMPERATURE: float = 0.3
        MAX_TOKENS: int = 1800
        OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY", "")
        SEARCH_API_URL: str = os.getenv("SEARCH_API_URL", "http://localhost:8008/check_and_search")

    def __init__(self):
        self.name = "Ğ­ĞºÑĞ¿ĞµÑ€Ñ‚ Ğ¿Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑĞ¼"
        self.valves = self.Valves()

    async def on_startup(self):
        logging.info("Pipeline is warming up...")

    async def on_shutdown(self):
        logging.info("Pipeline is shutting down...")

    async def make_request_with_retry(self, fn: Callable[[], Iterator[str]], retries=3) -> Iterator[str]:
        for attempt in range(retries):
            try:
                return fn()
            except Exception as e:
                logging.error(f"Attempt {attempt + 1} failed: {e}")
                if attempt + 1 == retries:
                    raise
                await asyncio.sleep(2 ** attempt)
    async def inlet(self, body: dict, user: dict) -> dict:
        import json

        logging.info(f"ğŸ“¥ Received inlet body:\n{json.dumps(body, indent=2, ensure_ascii=False)}")
        files = body.get("files", [])
        extracted_texts = []

        for file in files:
            content_url = file["url"] + "/content"
            async with httpx.AsyncClient(timeout=30) as client:
                response = await client.get(content_url)
                response.raise_for_status()
                content = response.content

            mime_type = file.get("mime_type") or mimetypes.guess_type(file.get("name", ""))[0]

            if mime_type == "application/pdf":
                doc = fitz.open(stream=content, filetype="pdf")
                text = "\n".join([page.get_text() for page in doc])
                extracted_texts.append(text)

            elif mime_type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
                with open("temp.docx", "wb") as f:
                    f.write(content)
                text = docx2txt.process("temp.docx")
                os.remove("temp.docx")
                extracted_texts.append(text)

            elif mime_type and mime_type.startswith("image/"):
                image = Image.open(io.BytesIO(content))
                from openai import OpenAI
                client = OpenAI(api_key=self.valves.OPENAI_API_KEY)
                base64_image = base64.b64encode(content).decode("utf-8")
                ocr_response = client.chat.completions.create(
                    model=self.valves.MODEL_ID,
                    messages=[
                        {"role": "user", "content": [
                            {"type": "text", "text": "Ğ Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸."},
                            {"type": "image_url", "image_url": {"url": f"data:{mime_type};base64,{base64_image}"}}
                        ]}
                    ]
                )
                image_text = ocr_response.choices[0].message.content.strip()
                extracted_texts.append(image_text)

        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚
        body["file_text"] = "\n".join(extracted_texts)

        # âœ… ĞĞ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ´Ğ»Ñ OpenWebUI
        if "messages" not in body or not isinstance(body["messages"], list):
            raise ValueError("Field 'messages' is required and must be a list.")
        if "model" not in body or not isinstance(body["model"], str):
            raise ValueError("Field 'model' is required and must be a string.")

        logging.info("âœ… inlet completed successfully.")
        return body
    async def call_search_api(self, prompt: str) -> Dict[str, Any]:
        try:
            async with httpx.AsyncClient(timeout=30) as client:
                response = await client.post(
                    self.valves.SEARCH_API_URL,
                    json={"prompt": prompt, "pipeline": "LawExp"}
                )
                response.raise_for_status()
                return response.json()
        except Exception as e:
            logging.error(f"Search API error: {e}")
            return {"search_required": False, "context": "", "citations": []}

    def pipe(
        self, user_message: str, model_id: str, messages: List[dict], body: dict
    ) -> Iterator[str]:
        file_text = body.get("file_text", "")
        if file_text:
            user_message += f"\n\nĞ¢ĞµĞºÑÑ‚ Ğ¸Ğ· Ğ¿Ñ€Ğ¸ĞºÑ€ĞµĞ¿Ğ»Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²:\n{file_text}"
        system_message = """
**Ğ Ğ¾Ğ»ÑŒ:** Ğ’Ñ‹ â€” Ğ˜Ğ˜-ÑĞºÑĞ¿ĞµÑ€Ñ‚ Ğ¿Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¾Ğ²Ğ¾Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¸Ğ·Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ´Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°.

**Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ°:**
1. ĞŸÑ€Ğ¾Ğ²ĞµÑÑ‚Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ° Ğ¸Ğ»Ğ¸ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ñ‹.
2. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ, Ğ½Ğµ Ğ´ÑƒĞ±Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»Ğ¸ Ğ¾Ğ½ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ´Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°.
3. Ğ’Ñ‹ÑĞ²Ğ¸Ñ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ½Ğ¾Ñ€Ğ¼Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒĞ¶Ğµ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ Ñ‚Ğµ Ğ¶Ğµ Ğ¼ĞµÑ€Ñ‹.
4. Ğ”Ğ°Ñ‚ÑŒ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ´ÑƒĞ±Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞµ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ñ‹.

**Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°:**

### ğŸ“„ ĞšÑ€Ğ°Ñ‚ĞºĞ¾Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ñ‹

- [1â€“2 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ]

### ğŸ” ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ Ğ±Ğ»Ğ¾ĞºĞ°Ğ¼
1. **ĞĞ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ€Ñ‹** â€” Ñ‡Ñ‚Ğ¾ ÑƒĞ¶Ğµ ĞµÑÑ‚ÑŒ Ğ² ĞĞ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ¼ ĞºĞ¾Ğ´ĞµĞºÑĞµ, Ğ¿ĞµÑ€ĞµÑĞµÑ‡ĞµĞ½Ğ¸Ñ.
2. **Ğ“Ñ€Ğ°Ğ½Ñ‚Ñ‹ Ğ¸ ÑÑƒĞ±ÑĞ¸Ğ´Ğ¸Ğ¸** â€” ĞµÑÑ‚ÑŒ Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸.
3. **Ğ ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ±Ğ¸Ğ·Ğ½ĞµÑĞ°** â€” Ğ´ÑƒĞ±Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ ĞšĞ¾Ğ´ĞµĞºÑĞ¾Ğ¼ Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğµ Ğ¸ Egov.

### âš–ï¸ Ğ—Ğ°ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ

- Ğ•ÑÑ‚ÑŒ Ğ»Ğ¸ Ğ´ÑƒĞ±Ğ»Ğ¸, Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ñ€Ğ¸ÑĞºĞ¸.
- Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ/ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ.
"""

        search_result = asyncio.run(self.call_search_api(user_message))
        deep_ctx = {}
        if search_result["search_required"] and search_result["citations"]:
            deep_ctx = asyncio.run(
                self.call_search_api(  # ğŸ‘ˆ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ½Ğ¾ Ğ¼ĞµĞ½ÑĞµĞ¼ path
                    user_message.replace(self.valves.SEARCH_API_URL,
                    self.valves.SEARCH_API_URL.replace("/check_and_search",
                    "/deep_extract_and_analyze")),
                )
            )
        enriched_prompt = user_message
        if search_result["search_required"] and search_result["context"]:
            enriched_prompt += "\n\nĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ· Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²:\n" + search_result["context"]
        if deep_ctx.get("legal_context"):
            enriched_prompt += "\n\nğŸ“˜ ĞšĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ½Ğ¾Ñ€Ğ¼Ñ‹ Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¾Ğº:\n" + deep_ctx["legal_context"]

        model = ChatOpenAI(
            api_key=self.valves.OPENAI_API_KEY,
            model=self.valves.MODEL_ID,
            temperature=self.valves.TEMPERATURE,
            streaming=True,
            max_tokens=self.valves.MAX_TOKENS
        )

        prompt = ChatPromptTemplate.from_messages([
            SystemMessagePromptTemplate.from_template(system_message),
            HumanMessagePromptTemplate.from_template("{user_input}")
        ])
        formatted_messages = prompt.format_messages(user_input=enriched_prompt)

        def generate_stream() -> Iterator[str]:
            for chunk in model.stream(formatted_messages):
                content = getattr(chunk, "content", None)
                if content:
                    yield content

            if search_result["search_required"] and search_result["citations"]:
                yield "\n\n### ğŸ“š Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸:"
                for i, link in enumerate(search_result["citations"], 1):
                    yield f"\n[{i}] {link}"

        return asyncio.run(self.make_request_with_retry(generate_stream))
