import logging
import sys
import os
import asyncio
from typing import List, Iterator, Callable, Any, Dict
from pydantic import BaseModel
import httpx
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate

logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))

class Pipeline:
    class Valves(BaseModel):
        MODEL_ID: str = "gpt-4o"
        TEMPERATURE: float = 0.3
        MAX_TOKENS: int = 1500
        OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY", "")
        SEARCH_API_URL: str = os.getenv("SEARCH_API_URL", "http://localhost:8008/check_and_search")

    def __init__(self):
        self.name = "–§–∞–∫—Ç—á–µ–∫–∏–Ω–≥-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç"
        self.valves = self.Valves()

    async def on_startup(self):
        logging.info("Pipeline is warming up...")

    async def on_shutdown(self):
        logging.info("Pipeline is shutting down...")

    async def make_request_with_retry(self, fn: Callable[[], Iterator[str]], retries=3) -> Iterator[str]:
        for attempt in range(retries):
            try:
                return fn()
            except Exception as e:
                logging.error(f"Attempt {attempt + 1} failed: {e}")
                if attempt + 1 == retries:
                    raise
                await asyncio.sleep(2 ** attempt)

    async def call_search_api(self, prompt: str) -> Dict[str, Any]:
        try:
            async with httpx.AsyncClient(timeout=30) as client:
                response = await client.post(
                    self.valves.SEARCH_API_URL,
                    json={"prompt": prompt, "pipeline": "Facts"}
                )
                response.raise_for_status()
                return response.json()
        except Exception as e:
            logging.error(f"Search API error: {e}")
            return {"search_required": False, "context": "", "citations": []}

    def pipe(
        self, user_message: str, model_id: str, messages: List[dict], body: dict
    ) -> Iterator[str]:

        system_message = """
**–†–æ–ª—å:** –í—ã ‚Äî –ò–ò-—Ñ–∞–∫—Ç—á–µ–∫–µ—Ä –Ω–∞—É—á–Ω–æ–π —Å–ª—É–∂–±—ã. –í–∞—à–∞ –∑–∞–¥–∞—á–∞ ‚Äî –ø—Ä–æ–≤–µ—Ä—è—Ç—å –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ:
- –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π,
- –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –∞–∫—Ç–æ–≤ –∏
- –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –ø–æ–ª—É—á–µ–Ω–Ω–æ–≥–æ –∏–∑ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω).

**–ü–æ–≤–µ–¥–µ–Ω–∏–µ:**
- –ï—Å–ª–∏ –≤—ã –Ω–µ —É–≤–µ—Ä–µ–Ω—ã –≤ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–∞–Ω–Ω—ã–µ, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –∏–∑ –≤–Ω–µ—à–Ω–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (`–∫–æ–Ω—Ç–µ–∫—Å—Ç`).
- –ù–µ –æ—Å—Ç–∞–≤–ª—è–π—Ç–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –±–µ–∑ –∞–Ω–∞–ª–∏–∑–∞. –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç ‚Äî –Ω–∞–ø–∏—à–∏—Ç–µ, —á—Ç–æ –æ–Ω–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –¥–∞–∂–µ –≤ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö.

**–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞:**

---

### üßæ –£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
<—Ü–∏—Ç–∞—Ç–∞ –∏–∑ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞>

---

### ‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–∫—Ç–∞
- –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ / –û–ø—Ä–æ–≤–µ—Ä–≥–Ω—É—Ç–æ / –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö

---

### üìä –ê–Ω–∞–ª–∏–∑
- –£–∫–∞–∂–∏—Ç–µ –∏—Å—Ç–æ—á–Ω–∏–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
- –£—Ç–æ—á–Ω–∏—Ç–µ —Å—Ç–µ–ø–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
- –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏: —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏, –≥–¥–µ –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ —Å–≤–µ–¥–µ–Ω–∏—è

---

**–û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–±–∞–≤–ª—è–π—Ç–µ —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, –µ—Å–ª–∏ –æ–Ω–∏ –±—ã–ª–∏ –ø–æ–ª—É—á–µ–Ω—ã.**
"""


        search_result = asyncio.run(self.call_search_api(user_message))

        if not search_result["search_required"] or not search_result["context"]:
            enriched_prompt = (
                f"–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–≤–µ—Ä—å —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ: \"{user_message}\". "
                "–ï—Å–ª–∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω—ã—Ö —Å–≤–µ–¥–µ–Ω–∏–π –Ω–µ—Ç ‚Äî —Å–æ–æ–±—â–∏—Ç–µ –æ–± —ç—Ç–æ–º –∏ –ø–æ—è—Å–Ω–∏—Ç–µ, –ø–æ—á–µ–º—É."
            )
        else:
            enriched_prompt = (
                f"–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–≤–µ—Ä—å —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ: \"{user_message}\". "
                "–ù–∏–∂–µ –ø—Ä–∏–≤–µ–¥—ë–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –µ–≥–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:\n\n"
                + search_result["context"]
            )


        model = ChatOpenAI(
            api_key=self.valves.OPENAI_API_KEY,
            model=self.valves.MODEL_ID,
            temperature=self.valves.TEMPERATURE,
            streaming=True,
            max_tokens=self.valves.MAX_TOKENS
        )

        prompt = ChatPromptTemplate.from_messages([
            SystemMessagePromptTemplate.from_template(system_message),
            HumanMessagePromptTemplate.from_template("{user_input}")
        ])
        formatted_messages = prompt.format_messages(user_input=enriched_prompt)

        def generate_stream() -> Iterator[str]:
            for chunk in model.stream(formatted_messages):
                content = getattr(chunk, "content", None)
                if content:
                    yield content

            if search_result["search_required"] and search_result["citations"]:
                yield "\n\n### üìö –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:"
                for i, link in enumerate(search_result["citations"], 1):
                    yield f"\n[{i}] {link}"

        return asyncio.run(self.make_request_with_retry(generate_stream))
