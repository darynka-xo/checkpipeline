import logging
import sys
import os
import asyncio
from typing import List, Iterator, Callable, Any, Dict
from pydantic import BaseModel
import httpx
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate

logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))


class Pipeline:
    class Valves(BaseModel):
        MODEL_ID: str = "gpt-4o"
        TEMPERATURE: float = 0.7
        MAX_TOKENS: int = 1500
        OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY", "")
        SEARCH_API_URL: str = os.getenv("SEARCH_API_URL", "http://localhost:8008/check_and_search")

    def __init__(self):
        self.name = "Negotiation Strategy Predictor with Evidence"
        self.valves = self.Valves()

    async def on_startup(self):
        logging.info("Pipeline is warming up...")

    async def on_shutdown(self):
        logging.info("Pipeline is shutting down...")

    async def make_request_with_retry(self, fn: Callable[[], Iterator[str]], retries=3) -> Iterator[str]:
        for attempt in range(retries):
            try:
                return fn()
            except Exception as e:
                logging.error(f"Attempt {attempt + 1} failed: {e}")
                if attempt + 1 == retries:
                    raise
                await asyncio.sleep(2 ** attempt)

    async def call_search_api(self, prompt: str) -> Dict[str, Any]:
        try:
            async with httpx.AsyncClient(timeout=20) as client:
                response = await client.post(
                    self.valves.SEARCH_API_URL,
                    json={"prompt": prompt, "pipeline": "NegotiationPipeline"}
                )
                response.raise_for_status()
                return response.json()
        except Exception as e:
            logging.error(f"Search API error: {e}")
            return {"search_required": False, "context": "", "citations": []}

    async def call_deep_extract_api(self, prompt: str, citations: List[str]) -> str:
        try:
            async with httpx.AsyncClient(timeout=60) as client:
                response = await client.post(
                    self.valves.SEARCH_API_URL.replace("/check_and_search", "/deep_extract_and_analyze"),
                    json={"prompt": prompt, "citations": citations, "pipeline": "NegotiationPipeline"}
                )
                response.raise_for_status()
                return response.json().get("legal_context", "")
        except Exception as e:
            logging.error(f"Deep extract API error: {e}")
            return ""

    async def inlet(self, body: dict, user: dict) -> dict:
        import json
        import base64
        import mimetypes
        import io
        import fitz
        from PIL import Image
        import docx2txt
        from openai import OpenAI

        logging.info(f"üì• Received inlet body:\n{json.dumps(body, indent=2, ensure_ascii=False)}")
        files = body.get("files", [])
        extracted_texts = []
        if files:
            for file in files:
                url = file.get("url", "")
                content = None
        
                if url.startswith("http://") or url.startswith("https://"):
                    content_url = url + "/content"
                    async with httpx.AsyncClient(timeout=30) as c:
                        resp = await c.get(content_url)
                        resp.raise_for_status()
                        content = resp.content
                elif url.startswith("data:"):
                    # –ü—Ä–∏–º–µ—Ä: data:application/vnd.openxmlformats-officedocument.wordprocessingml.document;base64,...
                    header, b64data = url.split(",", 1)
                    content = base64.b64decode(b64data)
                else:
                    logging.warning(f"‚ö†Ô∏è Unsupported or missing URL scheme: {url}")
                    continue
        
                mime = file.get("mime_type") or mimetypes.guess_type(file.get("name", ""))[0]
                if mime == "application/pdf":
                    doc = fitz.open(stream=content, filetype="pdf")
                    extracted.append("\n".join(p.get_text() for p in doc))
                elif mime == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
                    with open("_tmp.docx", "wb") as tmp:
                        tmp.write(content)
                    extracted.append(docx2txt.process("_tmp.docx"))
                    os.remove("_tmp.docx")
                elif mime and mime.startswith("image/"):
                    b64 = base64.b64encode(content).decode()
                    res = self.client.chat.completions.create(
                        model=self.valves.MODEL_ID,
                        messages=[{"role": "user", "content": [
                            {"type": "text", "text": "–†–∞—Å–ø–æ–∑–Ω–∞–π —Ç–µ–∫—Å—Ç –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏."},
                            {"type": "image_url", "image_url": {"url": f"data:{mime};base64,{b64}"}}
                        ]}]
                    )
                    extracted.append(res.choices[0].message.content.strip())
        
            body["file_text"] = "\n".join(extracted)
        else:
            body["file_text"] = ""
        return body

    def pipe(self, user_message: str, model_id: str, messages: List[dict], body: dict) -> Iterator[str]:
        file_text = body.get("file_text", "")
        if file_text:
            user_message += f"\n\n–¢–µ–∫—Å—Ç –∏–∑ –ø—Ä–∏–∫—Ä–µ–ø–ª—ë–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:\n{file_text}"

        search_result = asyncio.run(self.call_search_api(user_message))
        if search_result["search_required"] and search_result["context"]:
            user_message += "\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:\n" + search_result["context"]

        deep_legal_context = ""
        if search_result["search_required"] and search_result["citations"]:
            deep_legal_context = asyncio.run(self.call_deep_extract_api(user_message, search_result["citations"]))
            if deep_legal_context:
                user_message += ("\n\nüìò –ü–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã–µ –Ω–æ—Ä–º—ã –∑–∞–∫–æ–Ω–∞ (–∏–∑ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤):\n"
                                 f"{deep_legal_context}\n"
                                 "\n‚ùóÔ∏è–ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —ç—Ç–∏ –Ω–æ—Ä–º—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")

        system_message = f"""
**–†–æ–ª—å:** –í—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø–µ—Ä–µ–≥–æ–≤–æ—Ä–∞–º –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–º—É —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é. –í–∞—à–∞ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ‚Äî –∞–Ω–∞–ª–∏–∑ –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–µ–¥–µ–Ω–∏—è –ø–µ—Ä–µ–≥–æ–≤–æ—Ä–æ–≤, –∞ —Ç–∞–∫–∂–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –¥–ª—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –º–µ–∂–¥—É –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å—Ç–æ—Ä–æ–Ω–∞–º–∏.

**–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:**
{deep_legal_context or "‚Äî"}

**–û–±–ª–∞—Å—Ç—å –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏:**
–í—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–Ω—ã–µ –∏–Ω—Ç–µ—Ä–µ—Å—ã –º–µ–∂–¥—É —Å—Ç–æ—Ä–æ–Ω–∞–º–∏, –≤—ã–¥–µ–ª—è–µ—Ç–µ –∏—Ö —Ü–µ–ª–∏, –≤—ã—è–≤–ª—è–µ—Ç–µ —Å–∏–ª—å–Ω—ã–µ –∏ —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –∏—Ö –ø–æ–∑–∏—Ü–∏–π –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–∞.

**–¶–µ–ª—å:**
1. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–Ω—É—é —Å–∏—Ç—É–∞—Ü–∏—é —Å —É—á–∞—Å—Ç–∏–µ–º –¥–≤—É—Ö –∏–ª–∏ –±–æ–ª–µ–µ —Å—Ç–æ—Ä–æ–Ω.
2. –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∏ –ø–æ–∑–∏—Ü–∏–∏ –∫–∞–∂–¥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã.
3. –û—Ü–µ–Ω–∏—Ç—å —Å–ª–∞–±—ã–µ –∏ —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã —ç—Ç–∏—Ö –ø–æ–∑–∏—Ü–∏–π.
4. –ù–∞–π—Ç–∏ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å, —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è—é—â–∏–π –∏–Ω—Ç–µ—Ä–µ—Å–∞–º –≤—Å–µ—Ö —Å—Ç–æ—Ä–æ–Ω (–Ω–∞—Å–∫–æ–ª—å–∫–æ —ç—Ç–æ –≤–æ–∑–º–æ–∂–Ω–æ).

**–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞:**
### 1. –°—Ç–æ—Ä–æ–Ω—ã –ø–µ—Ä–µ–≥–æ–≤–æ—Ä–æ–≤
- –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –∏ –∏—Ö –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤.

### 2. –ü–æ–∑–∏—Ü–∏–∏ —Å—Ç–æ—Ä–æ–Ω
#### –°—Ç–æ—Ä–æ–Ω–∞ A:
- –û—Å–Ω–æ–≤–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è: ...
- –°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã: ...
- –°–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã: ...

#### –°—Ç–æ—Ä–æ–Ω–∞ B:
- –û—Å–Ω–æ–≤–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è: ...
- –°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã: ...
- –°–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã: ...

### 3. –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å—ã
- –ü–µ—Ä–µ—á–µ–Ω—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç —á–∞—Å—Ç–∏—á–Ω–æ –∏–ª–∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç—å –≤—Å–µ —Å—Ç–æ—Ä–æ–Ω—ã.
- –£–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ —Ç–æ, –∫–∞–∫–∏–µ –∏–Ω—Ç–µ—Ä–µ—Å—ã –∫–∞–∂–¥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã —É—á—Ç–µ–Ω—ã –∏ –∫–∞–∫–∏–µ –æ—Å—Ç–∞—é—Ç—Å—è —Å–ø–æ—Ä–Ω—ã–º–∏.

### 4. –ü—Ä–æ–≥–Ω–æ–∑ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏
- –ê–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤, –¥–æ–ª–≥–æ–≤–µ—á–Ω–æ—Å—Ç–∏ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–∞.

### 5. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
- –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ —Å –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ–º.
- –£—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è —Å—Ç–æ—Ä–æ–Ω (–µ—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ–ø–æ–ª–Ω–∞).
"""

        model = ChatOpenAI(
            api_key=self.valves.OPENAI_API_KEY,
            model=self.valves.MODEL_ID,
            temperature=self.valves.TEMPERATURE,
            streaming=True,
            max_tokens=self.valves.MAX_TOKENS
        )

        prompt = ChatPromptTemplate.from_messages([
            SystemMessagePromptTemplate.from_template(system_message),
            HumanMessagePromptTemplate.from_template("{user_input}")
        ])

        formatted_messages = prompt.format_messages(user_input=user_message)

        def generate_stream() -> Iterator[str]:
            for chunk in model.stream(formatted_messages):
                content = getattr(chunk, "content", None)
                if content:
                    yield content
            if search_result["search_required"] and search_result["citations"]:
                yield "\n\n### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:"
                for i, link in enumerate(search_result["citations"], 1):
                    yield f"\n[{i}] {link}"

        return asyncio.run(self.make_request_with_retry(generate_stream))
