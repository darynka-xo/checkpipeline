import logging
import sys
import os
import asyncio
import requests
from bs4 import BeautifulSoup
from typing import List, Iterator, Sequence, Callable
from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tools import tool, BaseTool
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain_core.messages import SystemMessage, HumanMessage

logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))


class WebSearchInput(BaseModel):
    query: str = Field(description="Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ")


@tool("search_kz_web", args_schema=WebSearchInput, return_direct=False)
def search_kz_web(query: str) -> str:
    """ĞŸĞ¾Ğ¸ÑĞº, Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ·Ğ°Ñ…ÑÑ‚Ğ°Ğ½ÑĞºĞ¸Ñ… Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²."""
    try:
        trusted_sites = [
            "site:senate.parlam.kz", "site:akorda.kz", "site:primeminister.kz",
            "site:otyrys.prk.kz", "site:senate-zan.prk.kz",
            "site:lib.prk.kz", "site:online.zakon.kz", "site:adilet.zan.kz",
            "site:legalacts.egov.kz", "site:egov.kz", "site:eotinish.kz"
        ]
        query_with_sites = f"{query} " + " OR ".join(trusted_sites)
        url = f"https://www.google.com/search?q={requests.utils.quote(query_with_sites)}&hl=ru"
        headers = {"User-Agent": "Mozilla/5.0"}
        html = requests.get(url, headers=headers, timeout=10).text
        soup = BeautifulSoup(html, "html.parser")

        sources = []
        for g in soup.select(".tF2Cxc")[:3]:
            title = g.select_one("h3")
            link = g.select_one("a")
            if title and link:
                page_url = link["href"]
                page_text = extract_text_from_url(page_url)
                sources.append({
                    "title": title.text,
                    "url": page_url,
                    "text": page_text
                })

        if not sources:
            return "ĞĞ¸Ñ‡ĞµĞ³Ğ¾ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ½Ğ° Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ñ…."

        summary = analyze_multiple_sources(sources)
        return "ğŸ“Š ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²:\n\n" + summary
    except Exception as e:
        return f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ¾Ğ¸ÑĞºĞ°: {str(e)}"



def extract_text_from_url(url: str) -> str:
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")
        paragraphs = soup.find_all("p")
        text = "\n".join(p.get_text(strip=True) for p in paragraphs)
        return text.strip() if text else "ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ."
    except Exception as e:
        return f"[ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°: {str(e)}]"


def analyze_external_text(text: str, source_url: str) -> str:
    try:
        if not text or "ĞÑˆĞ¸Ğ±ĞºĞ°" in text:
            return text

        model = ChatOpenAI(
            api_key=os.getenv("OPENAI_API_KEY", ""),
            model_name="gpt-4o",
            temperature=0.3,
            max_tokens=512
        )

        messages = [
            SystemMessage(
                content=(
                    "Ğ’Ñ‹ â€” ÑĞºÑĞ¿ĞµÑ€Ñ‚ Ğ¿Ğ¾ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸ĞºĞµ.\n"
                    "Ğ¡Ğ´ĞµĞ»Ğ°Ğ¹Ñ‚Ğµ 2â€“3 Ğ¾Ñ‡ĞµĞ½ÑŒ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸Ğ· Ğ¿Ñ€Ğ¸Ğ²ĞµĞ´Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°, "
                    "ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑĞ½Ğ°Ğ±Ğ´Ğ¸Ñ‚Ğµ Ğ¿Ğ¾ÑÑĞ½ĞµĞ½Ğ¸ĞµĞ¼. Ğ’ ĞºĞ¾Ğ½Ñ†Ğµ ĞĞ‘Ğ¯Ğ—ĞĞ¢Ğ•Ğ›Ğ¬ĞĞ Ğ´Ğ¾Ğ±Ğ°Ğ²ÑŒÑ‚Ğµ "
                    f"ÑÑ‚Ñ€Ğ¾ĞºÑƒ 'Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº: {source_url}'."
                )
            ),
            HumanMessage(content=text[:3500])  # Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ Ğ²Ñ‹Ğ±Ğ¸Ñ‚ÑŒÑÑ Ğ¸Ğ· token-Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ğ°
        ]

        result = model.invoke(messages)
        return result.content.strip()
    except Exception as e:
        return f"[ĞÑˆĞ¸Ğ±ĞºĞ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°: {str(e)}]"


def analyze_multiple_sources(sources: List[dict]) -> str:
    try:
        model = ChatOpenAI(
            api_key=os.getenv("OPENAI_API_KEY", ""),
            model="gpt-4o",
            temperature=0.5
        )

        # ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ 2â€“3 Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° Ñ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¾Ğ¼ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼
        combined_chunks = []
        for src in sources[:3]:
            clean_text = src['text'][:1000].strip()
            if clean_text:
                combined_chunks.append(f"[Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº: {src['url']}]\n{clean_text}")

        combined_text = "\n\n".join(combined_chunks)

        messages = [
            SystemMessage(content="""
Ğ’Ñ‹ â€” ÑĞºÑĞ¿ĞµÑ€Ñ‚ Ğ¿Ğ¾ Ğ¿ĞµÑ€ĞµĞ³Ğ¾Ğ²Ğ¾Ñ€Ğ°Ğ¼ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ.
ĞĞ¸Ğ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ²Ñ‹Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¸Ğ· Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ².
ĞĞ° Ğ¸Ñ… Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ´ĞµĞ»Ğ°Ğ¹Ñ‚Ğµ 2â€“3 ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ñ Ğ¿Ğ¾ÑÑĞ½ĞµĞ½Ğ¸ĞµĞ¼. 
ğŸ“ ĞĞ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑÑÑ‹Ğ»Ğ°Ğ¹Ñ‚ĞµÑÑŒ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ [Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº: URL] Ğ¿Ğ¾ÑĞ»Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°.
"""),
            HumanMessage(content=combined_text)
        ]

        result = model.invoke(messages)
        return result.content
    except Exception as e:
        return f"[ĞÑˆĞ¸Ğ±ĞºĞ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°: {str(e)}]"


class Pipeline:
    class Valves(BaseModel):
        MODEL_ID: str = "gpt-4o"
        TEMPERATURE: float = 0.7
        MAX_TOKENS: int = 1500
        OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY", "")

    def __init__(self):
        self.name = "Negotiation Strategy Predictor with Search"
        self.valves = self.Valves()

    async def on_startup(self):
        logging.info("Pipeline is warming up...")

    async def on_shutdown(self):
        logging.info("Pipeline is shutting down...")

    async def make_request_with_retry(self, fn: Callable[[], Iterator[str]], retries=3) -> Iterator[str]:
        for attempt in range(retries):
            try:
                return fn()
            except Exception as e:
                logging.error(f"Attempt {attempt + 1} failed: {e}")
                if attempt + 1 == retries:
                    raise
                await asyncio.sleep(2 ** attempt)

    def pipe(
        self, user_message: str, model_id: str, messages: List[dict], body: dict
    ) -> Iterator[str]:

        system_message = system_message = """
**Ğ Ğ¾Ğ»ÑŒ:** ÑĞºÑĞ¿ĞµÑ€Ñ‚ Ğ¿Ğ¾ Ğ¿ĞµÑ€ĞµĞ³Ğ¾Ğ²Ğ¾Ñ€Ğ°Ğ¼ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ.  
**ĞÑ‚Ğ²ĞµÑ‡Ğ°ĞµÑˆÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ñ‚ĞµĞ¼Ñ‹ Ğ¿ĞµÑ€ĞµĞ³Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ğ², ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ¾Ğ², ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ.**  
Ğ•ÑĞ»Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾, Ğ¡ĞĞĞ§ĞĞ›Ğ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ `search_kz_web`
Ğ¸ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ°Ğ·Ğ°Ñ…ÑÑ‚Ğ°Ğ½ÑĞºĞ¸Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚.

**Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° (ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞ¹ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸):**
### 1. Ğ’Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ
### 2. ĞŸÑ€Ğ¾Ğ³Ğ½Ğ¾Ğ· ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸
### 3. Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸
"""

        model = ChatOpenAI(
            api_key=self.valves.OPENAI_API_KEY,
            model=self.valves.MODEL_ID,
            temperature=self.valves.TEMPERATURE,
            streaming=True
        )

        tools: Sequence[BaseTool] = [search_kz_web]

        prompt = ChatPromptTemplate.from_messages([
            ("system", system_message),
            MessagesPlaceholder("chat_history"),
            ("user", "{input}"),
            MessagesPlaceholder("agent_scratchpad")
        ])
    
        agent = create_tool_calling_agent(model, tools, prompt)
        agent_executor = AgentExecutor(
            agent=agent,
            tools=tools,
            verbose=True,
            handle_parsing_errors=True,
            streaming=True
        )

        def stream_agent() -> Iterator[str]:
            for chunk in agent_executor.stream({
                "input": user_message,
                "chat_history": messages
            }):
                output = chunk.get("output")
                if output:
                    logging.debug(f"Agent chunk: {output}")
                    yield output

        return asyncio.run(self.make_request_with_retry(stream_agent))
